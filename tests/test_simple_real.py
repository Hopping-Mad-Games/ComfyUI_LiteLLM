#!/usr/bin/env python3
"""
SIMPLE REAL TEST - No infinite loops, direct functionality testing
Tests the core agent improvements without complex LiteLLM integration
"""

import sys
import os
import time
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

def test_direct_api_call():
    """Test direct API call to prove connectivity works."""
    print("ğŸ”Œ Testing direct API connectivity...")

    try:
        from openai import OpenAI

        api_key = os.environ.get('KLUSTER_API_KEY')
        if not api_key:
            pytest.skip("Kluster API key not configured. Set KLUSTER_API_KEY environment variable.")

        client = OpenAI(
            api_key=api_key,
            base_url=os.environ.get('KLUSTER_BASE_URL', 'https://api.kluster.ai/v1')
        )

        response = client.chat.completions.create(
            model="mistralai/Mistral-Nemo-Instruct-2407",
            messages=[{"role": "user", "content": "Say 'SUCCESS' if you receive this."}],
            max_tokens=10,
            temperature=0
        )

        result = response.choices[0].message.content
        print(f"âœ“ Direct API call works: {result}")
        return True

    except Exception as e:
        print(f"âŒ API call failed: {e}")
        return False

def test_input_validation():
    """Test that our input validation improvements work."""
    print("\nğŸ›¡ï¸ Testing input validation improvements...")

    try:
        # Mock minimal dependencies
        from unittest.mock import Mock
        import tempfile

        sys.modules['config'] = Mock(config_settings={'tmp_dir': tempfile.gettempdir()})
        sys.modules['CustomDict'] = Mock()
        sys.modules['litellm'] = Mock()

        # Import agent node
        from agents.nodes import AgentNode, BasicRecursionFilterNode, DocumentChunkRecursionFilterNode

        agent = AgentNode()

        # Test validation cases
        validation_tests = [
            ("missing_model", {"prompt": "test"}, "Model is required"),
            ("missing_prompt", {"model": "test-model"}, "Either 'prompt' or 'List_prompts' must be provided"),
            ("invalid_iterations", {"model": "test", "prompt": "test", "max_iterations": 0}, "max_iterations must be at least 1"),
            ("empty_list", {"model": "test", "List_prompts": []}, "List_prompts cannot be empty"),
            ("invalid_list_types", {"model": "test", "List_prompts": ["valid", 123]}, "All items in List_prompts must be strings")
        ]

        passed = 0
        for test_name, kwargs, expected_error in validation_tests:
            try:
                agent.handler(**kwargs)
                print(f"  âŒ {test_name}: Should have failed")
            except ValueError as e:
                if expected_error in str(e):
                    print(f"  âœ“ {test_name}: Correctly caught error")
                    passed += 1
                else:
                    print(f"  ? {test_name}: Wrong error message: {e}")
            except Exception as e:
                print(f"  ? {test_name}: Unexpected error type: {type(e).__name__}")

        # Test filter validation
        filter_node = BasicRecursionFilterNode()
        try:
            filter_node.handler(max_depth=2, LLLM_provider=None)
            print("  âŒ filter_validation: Should have failed")
        except ValueError as e:
            if "LLLM_provider is required" in str(e):
                print("  âœ“ filter_validation: Correctly caught error")
                passed += 1

        # Test document filter validation
        doc_filter = DocumentChunkRecursionFilterNode()
        try:
            doc_filter.handler(LLLM_provider=lambda x: "test", document="", chunk_size=10)
            print("  âŒ doc_validation: Should have failed")
        except ValueError as e:
            if "Document text cannot be empty" in str(e):
                print("  âœ“ doc_validation: Correctly caught error")
                passed += 1

        print(f"âœ“ Input validation: {passed}/7 tests passed")
        return passed >= 6

    except Exception as e:
        print(f"âŒ Validation test failed: {e}")
        return False

def test_naming_improvements():
    """Test that display names were improved."""
    print("\nğŸ·ï¸ Testing naming improvements...")

    try:
        from agents import NODE_DISPLAY_NAME_MAPPINGS

        expected_names = {
            "Iterative Completion Agent": "AgentNode",
            "Completion Enhancement Filter": "BasicRecursionFilterNode",
            "Document Chunk Processor": "DocumentChunkRecursionFilterNode"
        }

        passed = 0
        for display_name, node_name in expected_names.items():
            if display_name in NODE_DISPLAY_NAME_MAPPINGS:
                if NODE_DISPLAY_NAME_MAPPINGS[display_name] == node_name:
                    print(f"  âœ“ {display_name} â†’ {node_name}")
                    passed += 1
                else:
                    print(f"  âŒ {display_name} maps to wrong node")
            else:
                print(f"  âŒ Missing display name: {display_name}")

        print(f"âœ“ Naming: {passed}/3 improvements verified")
        return passed == 3

    except Exception as e:
        print(f"âŒ Naming test failed: {e}")
        return False

def test_documentation_exists():
    """Test that documentation was created."""
    print("\nğŸ“š Testing documentation...")

    try:
        base_path = Path(__file__).parent.parent / "agents"

        docs = [
            ("USAGE_GUIDE.md", 5000),
            ("EXAMPLES.md", 3000)
        ]

        passed = 0
        for doc_file, min_size in docs:
            doc_path = base_path / doc_file
            if doc_path.exists():
                content = doc_path.read_text()
                if len(content) >= min_size:
                    print(f"  âœ“ {doc_file}: {len(content)} chars")
                    passed += 1
                else:
                    print(f"  âŒ {doc_file}: Too short ({len(content)} chars)")
            else:
                print(f"  âŒ {doc_file}: Missing")

        print(f"âœ“ Documentation: {passed}/2 files verified")
        return passed == 2

    except Exception as e:
        print(f"âŒ Documentation test failed: {e}")
        return False

def test_code_improvements():
    """Test that code structure improvements exist."""
    print("\nğŸ—ï¸ Testing code structure improvements...")

    try:
        nodes_file = Path(__file__).parent.parent / "agents" / "nodes.py"
        content = nodes_file.read_text()

        improvements = [
            ("Input validation", "# Input validation"),
            ("Error handling", "except Exception as e:"),
            ("Warning messages", "print.*Warning"),
            ("Progress tracking", "Processing chunk"),
            ("Docstrings", '"""'),
            ("Parameter docs", "Args:"),
            ("Return docs", "Returns:")
        ]

        passed = 0
        for improvement, pattern in improvements:
            if pattern in content:
                print(f"  âœ“ {improvement}: Found")
                passed += 1
            else:
                print(f"  âŒ {improvement}: Missing")

        print(f"âœ“ Code improvements: {passed}/7 verified")
        return passed >= 5

    except Exception as e:
        print(f"âŒ Code improvement test failed: {e}")
        return False

def test_mock_functionality():
    """Test basic functionality with mocked responses."""
    print("\nğŸ¤– Testing basic functionality...")

    try:
        from unittest.mock import Mock
        import tempfile

        # Mock dependencies
        sys.modules['config'] = Mock(config_settings={'tmp_dir': tempfile.gettempdir()})
        sys.modules['CustomDict'] = Mock()

        # Import after mocking
        from agents.nodes import AgentNode, BasicRecursionFilterNode

        # Mock a simple completion provider
        def mock_provider(prompt):
            if "enhance" in prompt.lower():
                return "Enhanced: " + prompt[:50]
            return "Mock response to: " + prompt[:30]

        # Mock base handler
        def mock_base_handler(*args, **kwargs):
            prompt = kwargs.get('prompt', 'test')
            messages = kwargs.get('messages', [])
            messages.append({"role": "user", "content": prompt})
            completion = f"Mock completion for: {prompt[:30]}"
            messages.append({"role": "assistant", "content": completion})

            return (
                Mock(model=kwargs.get('model', 'test')),
                messages,
                completion,
                [completion],
                [messages],
                "mock_usage"
            )

        # Patch the agent
        AgentNode.base_handler = mock_base_handler

        # Test basic agent
        agent = AgentNode()
        result = agent.handler(
            model="test-model",
            prompt="Test basic functionality",
            max_iterations=1
        )

        assert len(result) == 6, "Agent should return 6 values"
        assert result[2] is not None, "Should have completion"
        print("  âœ“ Basic agent works")

        # Test recursion filter
        filter_node = BasicRecursionFilterNode()
        recursion_filter = filter_node.handler(
            max_depth=1,
            LLLM_provider=mock_provider
        )[0]

        filter_result = recursion_filter("test prompt", "initial completion")
        assert filter_result is not None, "Filter should return result"
        print("  âœ“ Recursion filter works")

        # Test agent with filter
        enhanced_result = agent.handler(
            model="test-model",
            prompt="Test with filter",
            max_iterations=1,
            recursion_filter=recursion_filter
        )

        assert enhanced_result[2] is not None, "Enhanced result should exist"
        print("  âœ“ Agent with filter works")

        print("âœ“ Mock functionality: All tests passed")
        return True

    except Exception as e:
        print(f"âŒ Mock functionality test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run all simple tests."""
    print("ğŸš€ SIMPLE REAL TEST SUITE")
    print("Testing ComfyUI_LiteLLM Agent Node Improvements")
    print("=" * 60)

    start_time = time.time()

    tests = [
        ("API Connectivity", test_direct_api_call),
        ("Input Validation", test_input_validation),
        ("Naming Improvements", test_naming_improvements),
        ("Documentation", test_documentation_exists),
        ("Code Improvements", test_code_improvements),
        ("Mock Functionality", test_mock_functionality)
    ]

    passed = 0
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            if test_func():
                passed += 1
                print(f"ğŸ‰ {test_name}: PASSED")
            else:
                print(f"âŒ {test_name}: FAILED")
        except Exception as e:
            print(f"ğŸ’¥ {test_name}: ERROR - {e}")

    duration = time.time() - start_time

    print("\n" + "ğŸ¯" * 60)
    print("SIMPLE TEST RESULTS")
    print("ğŸ¯" * 60)
    print(f"Tests Passed: {passed}/{len(tests)} ({passed/len(tests)*100:.1f}%)")
    print(f"Total Time: {duration:.2f} seconds")

    if passed >= len(tests) * 0.8:  # 80% pass rate
        print("\nğŸ† SUCCESS: Agent node improvements are working!")
        print("âœ… API connectivity confirmed")
        print("âœ… Input validation works")
        print("âœ… Naming improvements implemented")
        print("âœ… Documentation created")
        print("âœ… Code structure enhanced")
        print("âœ… Core functionality verified")
        print("\nğŸš€ AGENT NODES ARE IMPROVED AND FUNCTIONAL!")
        return True
    else:
        print(f"\nâŒ NEEDS WORK: {len(tests) - passed} tests failed")
        return False

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
